{"id":"review.meta.schema","phase":"meta","type":"schema","title":"Review Entry Schema","description":"Each entry follows this schema: id (review.{phase}.{n}), phase, group, type (learning|observation|concern|proposal|question), severity (info|low|medium|high|critical for concerns), priority (1-5 for proposals), title, description, evidence (array of file:line refs), related (array of other ids), timestamp, status (open|addressed|wontfix)","evidence":[],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.1","phase":"phase-1-mental-model","group":"architecture","type":"learning","severity":"info","title":"Domain-driven architecture is primary mental model","description":"DOPPLER uses domain-driven grouping (Compute/Data/Runtime) as the primary organizational model, with Build Layers as secondary. The ARCHITECTURE.md explicitly acknowledges that strict layering is approximate and shows actual dependency graph separately.","evidence":["docs/ARCHITECTURE.md:128-167","docs/ARCHITECTURE.md:168-194"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.2","phase":"phase-1-mental-model","group":"architecture","type":"learning","severity":"info","title":"GPU fusion is the core performance insight","description":"All tensor ops stay on GPU until final logits. Only ONE readback per generated token. JS orchestration overhead is ~2% of total time, making JS vs WASM irrelevant. This justifies the JS codebase choice.","evidence":["docs/ARCHITECTURE.md:55-64","docs/ARCHITECTURE.md:68-77"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.3","phase":"phase-1-mental-model","group":"architecture","type":"learning","severity":"info","title":"Accepts 20% kernel perf gap for capability gains","description":"DOPPLER explicitly accepts ~20% kernel performance gap vs TVM because it enables: 90GB MoE on 8GB VRAM (expert paging), P2P shard distribution, LoRA hot-swap, speculative decoding. These are impossible with compiled models.","evidence":["docs/ARCHITECTURE.md:79-89"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.4","phase":"phase-1-mental-model","group":"config","type":"learning","severity":"info","title":"Config-as-code with extensive schema system","description":"25+ schema files in config/schema/, each exporting DEFAULT_* constants. Two tiers: invariants (hardcoded) vs tunables (configurable). Runtime presets in config/presets/runtime/, model presets in config/presets/models/.","evidence":["src/config/schema/index.js:1-223"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.5","phase":"phase-1-mental-model","group":"config","type":"observation","severity":"info","title":"Schema exports are well-organized but numerous","description":"config/schema/index.js exports from 18 different schema files. Each schema file has multiple DEFAULT_* exports. This is comprehensive but creates a large API surface. Schema files cover: manifest, inference, loading, kernel, storage, debug, kvcache, moe, buffer-pool, memory-limits, bridge, adapter, quantization, kernel-thresholds, doppler master config.","evidence":["src/config/schema/index.js:1-223"],"related":["review.p1.4"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.6","phase":"phase-1-mental-model","group":"architecture","type":"learning","severity":"info","title":"Manifest-first architecture for model config","description":"All model-specific inference parameters embedded in manifest at conversion time. Manifest is single source of truth. Runtime can override but sources are tracked. No model detection (isGemma, isLlama) at runtime - happens once at conversion.","evidence":["docs/ARCHITECTURE.md:503-572"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.7","phase":"phase-1-mental-model","group":"inference","type":"learning","severity":"info","title":"Pipeline is thin orchestrator delegating to modules","description":"InferencePipeline extends PipelineState, uses PipelineGenerator. Sub-modules: state.js, generator.js, config.js, init.js (RoPE, KV cache, tokenizer, weights), debug-utils.js, layer-plan.js. Clean separation of concerns.","evidence":["src/inference/pipeline.js:1-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.8","phase":"phase-1-mental-model","group":"architecture","type":"observation","severity":"info","title":"Extensive auxiliary modules","description":"Beyond core inference: converter (Node-only), adapters (LoRA), bridge (extension IPC), hotswap (runtime swap), client (public API), browser (demo harness). These are well-isolated from core.","evidence":["docs/ARCHITECTURE.md:153-159"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.9","phase":"phase-1-mental-model","group":"architecture","type":"observation","severity":"info","title":"Directory structure matches domain model","description":"src/ subdirs align with domain grouping: gpu/ (Compute), formats/+loader/+storage/ (Data), inference/+config/ (Runtime). Memory/ and debug/ are true foundation with no internal deps.","evidence":[],"related":["review.p1.1"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.10","phase":"phase-1-mental-model","group":"architecture","type":"question","severity":"info","title":"Layer pipeline plans experimental status unclear","description":"ARCHITECTURE.md mentions 'Layer Pipeline Plans (experimental)' for per-layer step order customization via JSON. Unclear: is this actively used? What's the performance overhead vs hardcoded path? When should it be used?","evidence":["docs/ARCHITECTURE.md:411-462"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p1.11","phase":"phase-1-mental-model","group":"summary","type":"learning","severity":"info","title":"Phase 1 Complete: Mental model established","description":"Key takeaways: (1) Domain-driven architecture with Compute/Data/Runtime domains, (2) GPU-native pipeline with single readback, (3) Config-as-code with extensive schema system, (4) Manifest-first model config, (5) Clean module separation in pipeline. Ready for Phase 2 foundation deep dive.","evidence":[],"related":["review.p1.1","review.p1.2","review.p1.4","review.p1.6","review.p1.7"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.1","phase":"phase-2-foundation","group":"types","type":"learning","severity":"info","title":"Type system is minimal but well-organized","description":"5 .d.ts files total: index.d.ts (re-exports), model.d.ts (ModelConfig, ModelArchitecture, QuantFormat), gpu.d.ts (GpuCapabilities, KernelExecutor, buffer types), inference.d.ts (InferenceConfig, KVCache, TokenizerBackend). Clean separation of concerns. Supports 12 model architectures including llama, gemma, qwen, phi3, deepseek, mamba.","evidence":["src/types/model.d.ts:1-126","src/types/gpu.d.ts:1-186","src/types/inference.d.ts:1-201"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.2","phase":"phase-2-foundation","group":"types","type":"observation","severity":"info","title":"KernelExecutor interface defines core GPU operations","description":"KernelExecutor interface exposes: matmul, attention, softmax, rmsnorm, rope, flush. Three execution modes: immediate, recorded, batched. This is the abstraction layer between inference and kernels.","evidence":["src/types/gpu.d.ts:63-119"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.3","phase":"phase-2-foundation","group":"debug","type":"learning","severity":"info","title":"Debug system has two orthogonal dimensions","description":"Log levels (verbosity): silent < error < warn < info < verbose < debug. Trace categories (what): loader, kernels, logits, embed, attn, ffn, kv, sample, buffers, perf. Can combine: set log level verbose AND trace only 'attn,ffn'. CLI flags auto-map to URL params.","evidence":["src/debug/index.js:6-56","src/debug/log.js:75-158","src/debug/trace.js:76-216"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.4","phase":"phase-2-foundation","group":"debug","type":"observation","severity":"info","title":"Debug exposes global DOPPLER API in browser","description":"window.DOPPLER object provides runtime access to: trace, log, setLogLevel, setTrace, tensor.inspect, perf, history functions. Useful for interactive debugging. Auto-initializes from URL params on DOMContentLoaded.","evidence":["src/debug/index.js:172-214"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.5","phase":"phase-2-foundation","group":"debug","type":"observation","severity":"info","title":"Trace system supports layer filtering and decode step limits","description":"trace.attn(layerIdx, msg) and trace.ffn(layerIdx, msg) take layer index. Can filter to specific layers. traceMaxDecodeSteps limits output after N decode steps. Prevents log explosion during generation.","evidence":["src/debug/trace.js:25-39"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.6","phase":"phase-2-foundation","group":"memory","type":"learning","severity":"info","title":"Dual-strategy memory system: Memory64 vs Segmented","description":"Memory64: Single large WASM heap, supports >4GB if browser has Memory64. Segmented: Multiple 4GB ArrayBuffers with virtual addressing. HeapManager.allocate() returns unified AllocationResult with virtualAddress, view, strategy. Seamless fallback.","evidence":["src/memory/heap-manager.js:32-45","src/memory/heap-manager.js:169-233"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.7","phase":"phase-2-foundation","group":"memory","type":"learning","severity":"info","title":"Unified memory detection for Apple/AMD APUs","description":"Detects Apple Silicon (M1-M5) and AMD Strix Halo via WebGPU adapter info. Heuristics: vendor/device strings, large buffer limits (4GB+), platform detection. Enables larger models on unified memory systems (no PCIe copy overhead).","evidence":["src/memory/unified-detect.js:24-101","src/memory/unified-detect.js:141-218"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.8","phase":"phase-2-foundation","group":"memory","type":"observation","severity":"info","title":"Memory64 probing via WASM binary","description":"probeMemory64() compiles a minimal WASM module with memory64 flag to test browser support. Elegant feature detection without user-agent sniffing.","evidence":["src/memory/capability.js:25-41"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.9","phase":"phase-2-foundation","group":"config","type":"learning","severity":"info","title":"Hierarchical config with 13 sub-configs","description":"DEFAULT_RUNTIME_CONFIG aggregates: distribution, storage, loading, inference, kvcache, moe, bufferPool, gpuCache, tuner, memory, debug, hotSwap, bridge. Each is a separate schema file. createDopplerConfig factory for instantiation.","evidence":["src/config/schema/doppler.schema.js:19-33"],"related":["review.p1.4"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.10","phase":"phase-2-foundation","group":"config","type":"observation","severity":"info","title":"Deep merge with mergeRuntimeConfig","description":"mergeRuntimeConfig() does 2-level deep merge: top-level sections merged with spread, nested objects (like debug.trace, moe.routing) individually merged. Prevents overwriting entire sub-configs when only one field changes.","evidence":["src/config/schema/doppler.schema.js:65-146"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.11","phase":"phase-2-foundation","group":"config","type":"observation","severity":"info","title":"Runtime config is singleton with get/set/reset","description":"runtime.js maintains singleton runtimeConfig. getRuntimeConfig() returns current, setRuntimeConfig() merges overrides, resetRuntimeConfig() restores defaults. Includes deprecated maxTokens migration from sampling to batching.","evidence":["src/config/runtime.js:1-37"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.12","phase":"phase-2-foundation","group":"config","type":"concern","severity":"low","title":"Deprecated sampling.maxTokens migration logic","description":"runtime.js has migration code for sampling.maxTokens -> batching.maxTokens. This is technical debt that should eventually be removed once all configs are updated. Low severity as it's handled gracefully.","evidence":["src/config/runtime.js:18-27"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p2.13","phase":"phase-2-foundation","group":"summary","type":"learning","severity":"info","title":"Phase 2 Complete: Foundation is solid","description":"Key takeaways: (1) Types are minimal but sufficient with 5 files covering model/gpu/inference, (2) Debug system is sophisticated with orthogonal levels+categories, (3) Memory system handles browser heterogeneity elegantly, (4) Config is hierarchical with deep merge and singleton runtime. Foundation has no internal dependencies - truly standalone.","evidence":[],"related":["review.p2.1","review.p2.3","review.p2.6","review.p2.9"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.1","phase":"phase-3-critical-path","group":"generation","type":"learning","severity":"info","title":"Generation is async generator yielding tokens","description":"PipelineGenerator.generate() returns AsyncGenerator<string>. Yields tokens one at a time. Supports onToken callback per token, onBatch callback for batch mode. Stream-friendly API. Generation state tracked on PipelineState.","evidence":["src/inference/pipeline/generator.js:72-150"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.2","phase":"phase-3-critical-path","group":"generation","type":"learning","severity":"info","title":"Two-phase generation: prefill then decode","description":"_prefill(inputIds) processes all prompt tokens in parallel, returns logits for first token. Decode loop then generates tokens one at a time (or batched). Prefill is compute-bound, decode is memory-bound (KV cache read).","evidence":["src/inference/pipeline/generator.js:468-700"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.3","phase":"phase-3-critical-path","group":"generation","type":"observation","severity":"info","title":"Batch decode path for higher throughput","description":"When batchSize > 1 and GPU sampling available, uses _generateNTokensGPU() to generate multiple tokens before CPU readback. Falls back to single-token decode on error. stopCheckMode controls batch termination behavior.","evidence":["src/inference/pipeline/generator.js:163-213"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.4","phase":"phase-3-critical-path","group":"layer","type":"learning","severity":"info","title":"Layer processing: attention + FFN with residuals","description":"processLayer() dispatches to processLayerGPU() or processLayerCPU(). GPU path: RMSNorm -> Attention (QKV, RoPE, KV cache, softmax, output proj) -> Residual -> RMSNorm -> FFN (gate, up, activation, down) -> Residual.","evidence":["src/inference/pipeline/layer.js:92-200"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.5","phase":"phase-3-critical-path","group":"layer","type":"learning","severity":"info","title":"Sandwich norm detection for Gemma 3","description":"detectSandwichNorm() checks for preFeedforwardNorm, postFeedforwardNorm, postAttentionNorm weights. Gemma 3 uses sandwich norms (pre + post for each sublayer). Standard LLaMA uses single pre-norm.","evidence":["src/inference/pipeline/layer.js:42-53"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.6","phase":"phase-3-critical-path","group":"layer","type":"observation","severity":"info","title":"MoE layer detection per-layer","description":"isMoELayer() checks for router weights, then layer_types array, then defaults to all-MoE if model uses MoE. Allows hybrid dense+MoE architectures (some layers dense, some MoE).","evidence":["src/inference/pipeline/layer.js:62-76"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.7","phase":"phase-3-critical-path","group":"embed","type":"learning","severity":"info","title":"Embedding with optional Gemma scaling","description":"embed() calls gather kernel to lookup token embeddings. Optionally applies sqrt(hiddenSize) scaling for Gemma models. Supports F16 embeddings with F16 scale shader. Records to command recorder if batching.","evidence":["src/inference/pipeline/embed.js:1-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.8","phase":"phase-3-critical-path","group":"sampling","type":"learning","severity":"info","title":"CPU sampling with temperature, top-k, top-p","description":"sample() implements: greedy (temp=0), temperature scaling, softmax, top-k filtering, top-p (nucleus) filtering, renormalization, random sampling. applyRepetitionPenalty() penalizes recently seen tokens in sliding window.","evidence":["src/inference/pipeline/sampling.js:58-148"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.9","phase":"phase-3-critical-path","group":"generation","type":"observation","severity":"info","title":"Command recorder pattern for batching","description":"createCommandRecorder() or createProfilingRecorder() batches GPU operations. recorder.submitAndWait() flushes. Temporary buffers tracked for cleanup. Profiling recorder captures per-op timings via timestamp queries.","evidence":["src/inference/pipeline/generator.js:492-508"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.10","phase":"phase-3-critical-path","group":"generation","type":"observation","severity":"info","title":"Debug checkpoints force readbacks","description":"opts.debugLayers array specifies layer indices for checkpoint readbacks. At checkpoints, recorder is flushed and hidden state is read back for inspection. Allows debugging specific layers without full debug overhead.","evidence":["src/inference/pipeline/generator.js:561-617"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.11","phase":"phase-3-critical-path","group":"generation","type":"observation","severity":"info","title":"Logits health check with fallback","description":"After recorded logits GPU, checks for NaN/Inf/zero. If invalid, disables recorded logits and fused decode, falls back to unbatched computeLogits(). Self-healing behavior for edge cases.","evidence":["src/inference/pipeline/generator.js:676-697"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.12","phase":"phase-3-critical-path","group":"generation","type":"concern","severity":"low","title":"Multiple fallback paths increase complexity","description":"Generator has many fallback paths: batch->single-token, recorded->unbatched logits, GPU->CPU. Each adds code paths to test. Consider consolidating or simplifying fallback logic.","evidence":["src/inference/pipeline/generator.js:202-213","src/inference/pipeline/generator.js:676-697"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p3.13","phase":"phase-3-critical-path","group":"summary","type":"learning","severity":"info","title":"Phase 3 Complete: Token flow is clear","description":"Token flow: encode -> embed -> layers(attn+ffn) -> logits -> sample -> decode. Key patterns: (1) AsyncGenerator for streaming, (2) Prefill/decode split, (3) Command recorder batching, (4) GPU-native with single readback, (5) Sandwich norm for Gemma 3. One low concern: fallback complexity.","evidence":[],"related":["review.p3.1","review.p3.2","review.p3.4","review.p3.8","review.p3.9"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.1","phase":"phase-4a-kernel-infra","group":"device","type":"learning","severity":"info","title":"WebGPU device init with feature probing","description":"initDevice() requests adapter with fallback (high-perf -> low-power -> default). Probes for shader-f16, subgroups, subgroups-f16, timestamp-query. Requests max limits for storage buffers, workgroups. Caches device and KernelCapabilities. Device lost handler resets state.","evidence":["src/gpu/device.js:44-270"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.2","phase":"phase-4a-kernel-infra","group":"device","type":"learning","severity":"info","title":"Platform and kernel registry initialization","description":"initializePlatformAndRegistry() runs during device init. Dynamic imports config/platforms/loader.js and config/kernels/registry.js. Enables config-as-code kernel selection based on detected GPU. Non-fatal if fails.","evidence":["src/gpu/device.js:158-185"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.3","phase":"phase-4a-kernel-infra","group":"buffer-pool","type":"learning","severity":"info","title":"Power-of-2 bucket pooling with large buffer handling","description":"BufferPool uses power-of-2 buckets for small/medium buffers. Large buffers (>64MB default) use linear stepping to avoid 2x blowup (600MB->1GB OOM). Pools organized by usage flags and size bucket. Tracks stats for allocations, reuses, peak bytes.","evidence":["src/gpu/buffer-pool.js:41-75","src/gpu/buffer-pool.js:80-143"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.4","phase":"phase-4a-kernel-infra","group":"buffer-pool","type":"observation","severity":"info","title":"Buffer pool has debug mode for leak detection","description":"Debug mode tracks buffer metadata including stackTrace at acquisition time. Enables leak detection by comparing active buffers at shutdown. Deferred destruction queue for buffers destroyed after GPU work completes.","evidence":["src/gpu/buffer-pool.js:90-97"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.5","phase":"phase-4a-kernel-infra","group":"dispatch","type":"learning","severity":"info","title":"Dispatch utilities for all execution patterns","description":"dispatch.js provides: dispatch() for immediate single-op, recordDispatch() for batched, dispatchIndirect() for indirect args, dispatchMultiBindGroup() for multi-group, dispatchBatch() for multiple ops in one submit. Workgroup calculators for 1D/2D/3D.","evidence":["src/gpu/kernels/dispatch.js:1-209"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.6","phase":"phase-4a-kernel-infra","group":"constants","type":"learning","severity":"info","title":"Comprehensive GPU constants in single file","description":"constants.js defines: WORKGROUP_SIZES (256 default, 32 attention), MEMORY_THRESHOLDS (shared memory limits), DIMENSION_LIMITS (max seq len 32768, vocab 262144), TILE_SIZES, QUANTIZATION (Q4K 4.5 bits, block 144 bytes), ALIGNMENT (256 bytes), DTYPE_SIZES.","evidence":["src/gpu/kernels/constants.js:1-191"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.7","phase":"phase-4a-kernel-infra","group":"kernel-base","type":"observation","severity":"info","title":"KernelBase is minimal base class","description":"KernelBase holds device reference, provides getPipelineFor() and dispatchKernel()/recordKernel() methods. Very thin - most kernel logic is in individual kernel modules, not inherited.","evidence":["src/gpu/kernels/kernel-base.js:1-45"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.8","phase":"phase-4a-kernel-infra","group":"caching","type":"learning","severity":"info","title":"Three-level caching: shader source, module, pipeline","description":"shader-cache.js caches WGSL source (fetch) and compiled GPUShaderModule. pipeline-cache.js caches GPUComputePipeline, GPUBindGroupLayout, GPUPipelineLayout. Key format: 'operation:variant'. getPipelineFast() returns cached or creates.","evidence":["src/gpu/kernels/shader-cache.js:1-100","src/gpu/kernels/pipeline-cache.js:1-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.9","phase":"phase-4a-kernel-infra","group":"kernels","type":"learning","severity":"info","title":"Kernel modules follow consistent pattern","description":"Each kernel module exports: run{Kernel}() for immediate, record{Kernel}() for batched, select{Kernel}Kernel() for variant selection where applicable. 18+ kernel modules covering all ops: matmul, dequant, attention, rmsnorm, rope, silu, gelu, gather, residual, moe, cast, sample, fused ops.","evidence":["src/gpu/kernels/index.js:1-196"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.10","phase":"phase-4a-kernel-infra","group":"kernels","type":"observation","severity":"info","title":"Fused kernels for performance","description":"Three fused kernel modules: fused_ffn.js (FFN), fused_matmul_rmsnorm.js (matmul+norm for 1.2-1.5x decode speedup), fused_matmul_residual.js (matmul+residual). Each has shouldUseFused*() predicate.","evidence":["src/gpu/kernels/index.js:130-153"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4a.11","phase":"phase-4a-kernel-infra","group":"summary","type":"learning","severity":"info","title":"Phase 4A Complete: Kernel infrastructure is well-architected","description":"Key patterns: (1) Feature probing at init with fallbacks, (2) Power-of-2 pooling with large buffer handling, (3) Three-level caching (source/module/pipeline), (4) Consistent run/record API across kernels, (5) Fused kernels for performance. Clean separation of concerns.","evidence":[],"related":["review.p4a.1","review.p4a.3","review.p4a.8","review.p4a.9","review.p4a.10"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.1","phase":"phase-4b-simple-kernels","group":"gather","type":"learning","severity":"info","title":"Gather has 8 variants via lookup table","description":"selectGatherVariant() uses lookup table for F16 input/output and vec4 combinations. 8 variants total: default, vec4, f16, f16_vec4, f16_out, vec4_f16_out, f16_f16_out, f16_vec4_f16_out. Output binding varies by variant.","evidence":["src/gpu/kernels/gather.js:17-45"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.2","phase":"phase-4b-simple-kernels","group":"gather","type":"observation","severity":"info","title":"Gather supports transpose and indirect dispatch","description":"Options include transpose flag for transposed embedding matrix (lm_head reuse), indirectBuffer for dynamic dispatch counts. Returns typed Tensor wrapping output buffer.","evidence":["src/gpu/kernels/gather.js:48-130"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.3","phase":"phase-4b-simple-kernels","group":"rmsnorm","type":"learning","severity":"info","title":"RMSNorm has 8+ variants with capability detection","description":"selectRMSNormKernel() selects: default, small, subgroup, small_subgroup, residual, residual_small, default_f16, small_f16. Uses getKernelThresholds().rmsnorm.smallThreshold and capability hasSubgroups. Subgroup variants for efficient reduction.","evidence":["src/gpu/kernels/rmsnorm.js:19-56"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.4","phase":"phase-4b-simple-kernels","group":"rmsnorm","type":"observation","severity":"info","title":"RMSNorm residual variant fuses add+norm","description":"Residual variants take residual buffer, fuse x+residual with normalization. Reduces memory bandwidth. Placeholder buffer created when no residual (shader uses uniform flag to skip).","evidence":["src/gpu/kernels/rmsnorm.js:105-127"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.5","phase":"phase-4b-simple-kernels","group":"silu","type":"learning","severity":"info","title":"SiLU supports gate variants for SwiGLU","description":"SiLU variants: default, vec4, gate. Gate variant computes SiLU(gate)*up for SwiGLU activation. Row-split variants handle [gate..., up...] layout. F16 variants for each base.","evidence":["src/gpu/kernels/silu.js:15-28","src/gpu/kernels/silu.wgsl:46-58"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.6","phase":"phase-4b-simple-kernels","group":"silu","type":"observation","severity":"info","title":"WGSL shaders use multiple entry points per file","description":"silu.wgsl has 5 entry points: main (basic SiLU), silu_gate, silu_gate_split, silu_vec4, silu_gate_rowsplit. Each uses same Uniforms struct and bindings. Override for workgroup size. Clean sigmoid with clamping to prevent NaN.","evidence":["src/gpu/kernels/silu.wgsl:24-108"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.7","phase":"phase-4b-simple-kernels","group":"rope","type":"learning","severity":"info","title":"RoPE modifies input in-place","description":"runRoPE() applies rotary embeddings to Q/K tensors in-place (no separate output buffer). Uses precomputed cos/sin frequency buffers. Validates headDim is even. Schema-driven defaults for theta.","evidence":["src/gpu/kernels/rope.js:14-74"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.8","phase":"phase-4b-simple-kernels","group":"rope","type":"observation","severity":"info","title":"RoPE uniform struct matches WGSL exactly","description":"Uniform buffer layout (32 bytes): seqLen, numHeads, headDim, startPos, ropeBase (theta), ropeScale, 2 padding. Uses getKernelThresholds().rope.uniformSize from schema.","evidence":["src/gpu/kernels/rope.js:35-48"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.9","phase":"phase-4b-simple-kernels","group":"patterns","type":"learning","severity":"info","title":"Simple kernels share consistent patterns","description":"All simple kernels: (1) canUseF16() dtype check, (2) variant lookup table or select function, (3) getPipelineFast() for cached pipeline, (4) createUniformBufferWithView() for uniforms, (5) acquireBuffer() for output, (6) dispatch/recordDispatch, (7) createTensor() return.","evidence":["src/gpu/kernels/gather.js","src/gpu/kernels/rmsnorm.js","src/gpu/kernels/silu.js","src/gpu/kernels/rope.js"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4b.10","phase":"phase-4b-simple-kernels","group":"summary","type":"learning","severity":"info","title":"Phase 4B Complete: Simple kernels follow clean patterns","description":"Consistent JS wrapper pattern across kernels. WGSL uses multi-entry-point files. Key optimizations: vec4 for memory bandwidth, subgroups for reductions, F16 for throughput, fused residual for reduced passes. Schema-driven thresholds for variant selection.","evidence":[],"related":["review.p4b.1","review.p4b.3","review.p4b.6","review.p4b.9"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.1","phase":"phase-4c-complex-kernels","group":"matmul","type":"learning","severity":"info","title":"Matmul supports F32/F16/mixed/Q4K precision","description":"selectMatmulKernel() returns: f32, f16, f16_vec4, f16w_f32a (mixed). Q4K variants: q4_fused_multicol, q4_fused_batched, with F16 output option. isFusedQ4KDisabled() checks debug flag and kernel path config.","evidence":["src/gpu/kernels/matmul.js:20-84"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.2","phase":"phase-4c-complex-kernels","group":"matmul","type":"observation","severity":"info","title":"Matmul auto-detects transpose from WeightBuffer layout","description":"resolveTransposeB() checks WeightBuffer layout property. Column-major weights don't need transpose. Row-major weights need transposeB=true. Debug logging for first 50 calls.","evidence":["src/gpu/kernels/matmul.js:109-125"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.3","phase":"phase-4c-complex-kernels","group":"matmul","type":"observation","severity":"info","title":"Matmul has thorough validation","description":"validateMatmulDimensions() checks M/N/K are positive finite. validateMatmulOffsets() checks 256-byte alignment. getMatmulBindingSizes() validates buffer sizes with Q4K block size awareness.","evidence":["src/gpu/kernels/matmul.js:128-191"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.4","phase":"phase-4c-complex-kernels","group":"attention","type":"learning","severity":"info","title":"Attention has tiered kernel selection","description":"selectAttentionTier() selects: subgroup (best for decode with subgroup support), tiled_large (headDim<=64), tiled_small (headDim<=256), streaming (fallback). Checks shared memory limits against MEMORY_THRESHOLDS.","evidence":["src/gpu/kernels/attention.js:77-148"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.5","phase":"phase-4c-complex-kernels","group":"attention","type":"learning","severity":"info","title":"Attention variants for decode vs prefill and F16 KV","description":"resolveAttentionVariant() returns: decode, decode_subgroup, decode_chunked_f16kv, decode_streaming, prefill, prefill_small, etc. Chunked kernel for F16 KV with large headDim and bounded KV length.","evidence":["src/gpu/kernels/attention.js:154-209"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.6","phase":"phase-4c-complex-kernels","group":"attention","type":"observation","severity":"info","title":"Attention supports GQA and sliding window","description":"WGSL kernel has get_kv_head_idx() for grouped query attention (numKVHeads < numHeads). is_masked() handles causal + sliding window masking. Supports attn_softcap for Gemma 2.","evidence":["src/gpu/kernels/attention_decode.wgsl:41-67"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.7","phase":"phase-4c-complex-kernels","group":"attention","type":"observation","severity":"info","title":"Attention uses online softmax for memory efficiency","description":"Flash Attention-inspired: doesn't materialize full attention matrix. row_max and row_sum in workgroup shared memory for numerically stable online softmax. MAX_HEAD_DIM=64 compile-time constant.","evidence":["src/gpu/kernels/attention_decode.wgsl:35-39","src/gpu/kernels/attention_decode.wgsl:1-8"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.8","phase":"phase-4c-complex-kernels","group":"fused_ffn","type":"learning","severity":"info","title":"Fused FFN combines gate+up+activation","description":"Single kernel does: gate projection, up projection, SiLU activation, element-wise multiply. selectFFNVariant() returns: default, multi, batched, f16, q4k, q4k_batched. Reduces memory bandwidth vs separate ops.","evidence":["src/gpu/kernels/fused_ffn.js:30-51","src/gpu/kernels/fused_ffn.js:75-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.9","phase":"phase-4c-complex-kernels","group":"fused_ffn","type":"observation","severity":"info","title":"Fused FFN requires f32 activations with matching gate/up dtypes","description":"Validates input.dtype === 'f32', gate and up must have same dtype (f16, f32, or q4k). Multi-column workgroup layout for Q4K: 32 columns per workgroup.","evidence":["src/gpu/kernels/fused_ffn.js:91-106","src/gpu/kernels/fused_ffn.js:147-150"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4c.10","phase":"phase-4c-complex-kernels","group":"summary","type":"learning","severity":"info","title":"Phase 4C Complete: Complex kernels are sophisticated but well-structured","description":"Complex kernels (matmul, attention, FFN) have: (1) Tiered/variant selection based on capabilities, (2) Thorough validation, (3) Q4K fused paths for quantized models, (4) GQA and sliding window support, (5) Flash Attention principles. No major concerns found.","evidence":[],"related":["review.p4c.1","review.p4c.4","review.p4c.8"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4d.1","phase":"phase-4d-inference-kernel","group":"orchestration","type":"learning","severity":"info","title":"ops.js provides run/record abstraction layer","description":"ops.js wraps kernels with do{Op}() functions (doRMSNorm, doMatmul, doResidualAdd, doSiLU). Dispatches to run* or record* based on recorder presence. Handles kernel tracing and buffer lifecycle. Clean separation of orchestration from kernel details.","evidence":["src/inference/pipeline/ops.js:48-95"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4d.2","phase":"phase-4d-inference-kernel","group":"attention","type":"learning","severity":"info","title":"Attention orchestration handles F16 casting and LoRA","description":"runLayerAttentionGPU: casts F16->F32 if needed for attention, applies input norm, projects QKV, applies RoPE, runs attention, applies output projection. LoRA applied after each projection. Debug checkpoints at each stage.","evidence":["src/inference/pipeline/attention/run.js:50-150"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4d.3","phase":"phase-4d-inference-kernel","group":"ffn","type":"learning","severity":"info","title":"FFN orchestration with gateUp fusion","description":"runDenseFFNGPU: uses combined gate_up projection (single matmul for 2*intermediate), then SiLURowSplit for gated activation, then down projection. LoRA applied after each projection. Tracks temporary buffers for cleanup.","evidence":["src/inference/pipeline/ffn/dense.js:36-150"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4d.4","phase":"phase-4d-inference-kernel","group":"orchestration","type":"observation","severity":"info","title":"releaseOrTrack handles recorder vs immediate cleanup","description":"releaseOrTrack() checks if buffer is owned by DecodeBufferManager, then either tracks for recorder cleanup or releases immediately. Prevents leaks and double-frees. Clean buffer lifecycle management.","evidence":["src/inference/pipeline/ops.js:28-46"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p4d.5","phase":"phase-4d-inference-kernel","group":"summary","type":"learning","severity":"info","title":"Phase 4D Complete: Clean kernel orchestration","description":"Inference orchestrates kernels through ops.js abstraction. Key patterns: (1) run/record dispatch based on recorder, (2) F16 casting at layer boundaries, (3) LoRA applied per-projection, (4) releaseOrTrack for buffer lifecycle. Architecture is clean and maintainable.","evidence":[],"related":["review.p4d.1","review.p4d.2","review.p4d.3","review.p4d.4"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.1","phase":"phase-5-cross-cutting","group":"loader","type":"learning","severity":"info","title":"DopplerLoader orchestrates complete loading pipeline","description":"DopplerLoader class orchestrates: OPFS shard loading, Memory64/segmented heap staging, GPU buffer transfer. Tracks loaded state for embeddings, layers, experts, lmHead, finalNorm. Supports fused Q4K mode and column-wise layout. Uses helper modules: shard-resolver, tensor-loader, layer-loader, expert-loader.","evidence":["src/loader/doppler-loader.js:75-150"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.2","phase":"phase-5-cross-cutting","group":"loader","type":"observation","severity":"info","title":"DopplerLoader has modular helper structure","description":"Loading logic split into focused modules: shard-resolver.js (tensor locations), tensor-loader.js (GPU/CPU loading), layer-loader.js, embedding-loader.js, final-weights-loader.js, expert-loader.js, lora-loader.js. Clean separation of concerns.","evidence":["src/loader/doppler-loader.js:40-61"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.3","phase":"phase-5-cross-cutting","group":"storage","type":"learning","severity":"info","title":"Shard manager uses BLAKE3/SHA256 verification","description":"shard-manager.js handles OPFS with 4KB alignment for performance. BLAKE3 WASM for hashing (falls back to SHA-256). Supports FileSystemSyncAccessHandle for workers. Configurable via setOpfsPathConfig().","evidence":["src/storage/shard-manager.js:75-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.4","phase":"phase-5-cross-cutting","group":"formats","type":"learning","severity":"info","title":"Format parsers for GGUF, SafeTensors, RDRR","description":"formats/ has three parser modules: gguf/ (llama.cpp format, streaming for >2GB), safetensors/ (HuggingFace), rdrr/ (DOPPLER native manifest). GGUF parser reads only header+metadata (not tensor data). RDRR has validation, classification, groups modules.","evidence":["src/formats/gguf/parser.js:15-43"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.5","phase":"phase-5-cross-cutting","group":"quantization","type":"learning","severity":"info","title":"Dequantization supports Q4K, Q6K, Q8, MXFP4","description":"dequant.js: selectDequantKernel() chooses subgroup vs shared, vec4 vs scalar, F16 output. Supports Q4K (256-element blocks), Q6K, Q8_0, MXFP4 (32-element groups). Workgroup calculation handles MAX_WORKGROUPS limit.","evidence":["src/gpu/kernels/dequant.js:13-55","src/gpu/kernels/dequant.js:79-137"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.6","phase":"phase-5-cross-cutting","group":"quantization","type":"observation","severity":"info","title":"Two quantization paths: dequant vs fused Q4K","description":"Standard path: load quantized -> dequant kernel -> F32 -> matmul. Fused path: load quantized -> fused_matmul_q4_batched (no dequant). isFusedQ4KDisabled() controls which path. Fused is faster but requires column-wise layout.","evidence":["src/gpu/kernels/matmul.js:34-45","src/loader/doppler-loader.js:131-137"],"related":["review.p4c.1"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p5.7","phase":"phase-5-cross-cutting","group":"summary","type":"learning","severity":"info","title":"Phase 5 Complete: Clean data loading pipeline","description":"Data flow: OPFS (BLAKE3 verified) -> Heap (Memory64/Segmented) -> GPU. Format parsing for GGUF/SafeTensors/RDRR. Quantization via dequant kernel or fused Q4K path. Modular loader structure. No concerns found - well-designed.","evidence":[],"related":["review.p5.1","review.p5.3","review.p5.5","review.p5.6"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p6.1","phase":"phase-6-auxiliary","group":"adapters","type":"learning","severity":"info","title":"AdapterManager supports runtime LoRA switching","description":"AdapterManager class: loads adapters from path, tracks enabled/disabled state, supports multiple adapter stacking with merge strategies. Event callbacks for state changes. Weight parameter per adapter. No model reload required.","evidence":["src/adapters/adapter-manager.js:30-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p6.2","phase":"phase-6-auxiliary","group":"bridge","type":"learning","severity":"info","title":"Extension bridge for native file access","description":"ExtensionBridgeClient: connects to Chrome extension via runtime.connect. Binary message passing with request/response correlation. Commands for READ, LIST. Backpressure handling. Enables local file access without server.","evidence":["src/bridge/extension-client.js:50-100"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p6.3","phase":"phase-6-auxiliary","group":"hotswap","type":"learning","severity":"info","title":"Hot-swap with ECDSA signature verification","description":"hotswap/manifest.js: fetches manifests, verifies ECDSA P-256 signatures via WebCrypto. Trusted signers configured via policy. Local-only mode can skip signatures. Stable JSON serialization for signing.","evidence":["src/hotswap/manifest.js:24-69"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p6.4","phase":"phase-6-auxiliary","group":"converter","type":"learning","severity":"info","title":"Converter is Node-only CLI tool","description":"node-converter.js: facade re-exporting from node-converter/ module. Converts HuggingFace/GGUF to RDRR format. Separate from browser runtime. Uses modular architecture in node-converter/ directory.","evidence":["src/converter/node-converter.js:1-25"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p6.5","phase":"phase-6-auxiliary","group":"client","type":"observation","severity":"info","title":"DOPPLER provider for remote model API","description":"client/doppler-provider/: provider.js, model-manager.js, generation.js. Provides API for DOPPLER-as-a-service. Model manager handles remote model lifecycle. Generation module handles streaming responses.","evidence":["src/client/doppler-provider/*.js"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.p6.6","phase":"phase-6-auxiliary","group":"summary","type":"learning","severity":"info","title":"Phase 6 Complete: Auxiliary modules are well-isolated","description":"Auxiliary modules (adapters, bridge, hotswap, converter, client) are cleanly separated from core inference. Each has focused responsibility. Adapter stacking, extension bridge, signature verification, Node converter all work independently.","evidence":[],"related":["review.p6.1","review.p6.2","review.p6.3","review.p6.4","review.p6.5"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.final.1","phase":"final","group":"summary","type":"learning","severity":"info","title":"DOPPLER Codebase Review Complete","description":"Reviewed ~524 files across 6 phases: (1) Mental model - domain-driven architecture, (2) Foundation - types/debug/memory/config, (3) Critical path - token flow, (4) Kernels - infra/simple/complex, (5) Cross-cutting - loader/formats/quant, (6) Auxiliary - adapters/bridge/hotswap/converter. Total: 88 findings.","evidence":["reviews/codebase-audit.jsonl"],"related":[],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.final.2","phase":"final","group":"concerns","type":"concern","severity":"low","title":"Summary: 3 low-severity concerns found","description":"(1) review.p2.12: Deprecated sampling.maxTokens migration logic - technical debt, (2) review.p3.12: Multiple fallback paths in generator increase complexity, (3) No kernel concerns - well-architected. Overall codebase quality is high.","evidence":[],"related":["review.p2.12","review.p3.12"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}
{"id":"review.final.3","phase":"final","group":"architecture","type":"observation","severity":"info","title":"Architecture strengths","description":"Key architectural strengths: (1) Domain-driven grouping (Compute/Data/Runtime), (2) Config-as-code with schema system, (3) GPU-native pipeline with single readback, (4) Consistent kernel patterns (run/record), (5) Modular loader with helper modules, (6) Clean auxiliary module isolation.","evidence":[],"related":["review.p1.1","review.p1.2","review.p4a.9","review.p5.2"],"timestamp":"2026-01-09T00:00:00Z","status":"open"}