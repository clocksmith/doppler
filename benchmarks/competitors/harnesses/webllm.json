{
  "schemaVersion": 1,
  "id": "webllm",
  "name": "WebLLM Harness",
  "execution": {
    "mode": "external-command",
    "stdoutFormat": "json",
    "defaultCommand": null,
    "examples": [
      "node external/webllm-bench.js --workload decode-64-128-greedy --json",
      "python external/webllm_runner.py --model Llama-3.1-8B-Instruct-q4f16_1-MLC --json"
    ]
  },
  "normalization": {
    "metricPaths": {
      "decode_tokens_per_sec": ["metrics.decode_tokens_per_sec", "decode.tokens_per_sec", "decode_tps"],
      "prefill_tokens_per_sec": ["metrics.prefill_tokens_per_sec", "prefill.tokens_per_sec", "prefill_tps"],
      "ttft_ms": ["metrics.ttft_ms", "latency.ttft_ms", "ttft_ms"],
      "decode_ms_per_token_p50": ["metrics.decode_ms_per_token_p50", "latency.decode_p50_ms"],
      "decode_ms_per_token_p95": ["metrics.decode_ms_per_token_p95", "latency.decode_p95_ms"],
      "peak_memory_mb": ["metrics.peak_memory_mb", "memory.peak_mb"]
    },
    "metadataPaths": {
      "version": ["env.version", "version"],
      "model": ["model.id", "model"],
      "browser": ["env.browser", "browser.name"]
    },
    "requiredMetrics": [
      "decode_tokens_per_sec",
      "prefill_tokens_per_sec",
      "ttft_ms"
    ]
  }
}
