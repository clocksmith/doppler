{
  "id": "transformer",
  "name": "Base Transformer",
  "modelType": "transformer",

  "inference": {
    "attention": {
      "slidingWindow": null,
      "attnLogitSoftcapping": null,
      "queryKeyNorm": false,
      "ropeScalingType": null
    },
    "normalization": {
      "rmsNormWeightOffset": false,
      "rmsNormEps": 1e-5,
      "postAttentionNorm": false,
      "preFeedforwardNorm": false,
      "postFeedforwardNorm": false
    },
    "ffn": {
      "activation": "silu",
      "gatedFFN": true,
      "fusedGateUp": false
    },
    "output": {
      "finalLogitSoftcapping": null,
      "tieWordEmbeddings": false
    },
    "layerPattern": {
      "type": "all_attention"
    }
  },

  "tokenizer": {
    "addBosToken": true,
    "addEosToken": false
  },

  "sampling": {
    "temperature": 1.0,
    "topK": 50,
    "topP": 0.95,
    "repetitionPenalty": 1.0,
    "maxTokens": 2048
  },

  "tensorPatterns": {
    "embedding": [
      "model.embed_tokens.weight",
      "embeddings.word_embeddings.weight",
      "token_embd.weight"
    ],
    "lmHead": [
      "lm_head.weight",
      "output.weight"
    ],
    "attention": {
      "qProj": ["layers.{layer}.self_attn.q_proj.weight"],
      "kProj": ["layers.{layer}.self_attn.k_proj.weight"],
      "vProj": ["layers.{layer}.self_attn.v_proj.weight"],
      "oProj": ["layers.{layer}.self_attn.o_proj.weight"]
    },
    "ffn": {
      "gate": ["layers.{layer}.mlp.gate_proj.weight"],
      "up": ["layers.{layer}.mlp.up_proj.weight"],
      "down": ["layers.{layer}.mlp.down_proj.weight"]
    },
    "norm": {
      "input": ["layers.{layer}.input_layernorm.weight"],
      "postAttn": ["layers.{layer}.post_attention_layernorm.weight"],
      "final": ["model.norm.weight", "norm.weight"]
    }
  }
}
