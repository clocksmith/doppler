{
    "id": "gemma3-f16-fused-f32a-online",
    "name": "Gemma 3 F16 (F32 activations, fused online)",
    "description": "F16 weights with F32 activations to prevent overflow, utilizing fused FFN operators for bandwidth savings.",
    "activationDtype": "f32",
    "kvDtype": "f16",
    "decode": {
        "steps": [
            {
                "op": "input_norm",
                "kernel": "rmsnorm.wgsl",
                "entry": "main"
            },
            {
                "op": "q_proj",
                "kernel": "matmul_gemv_subgroup.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.q_proj"
            },
            {
                "op": "k_proj",
                "kernel": "matmul_gemv_subgroup.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.k_proj"
            },
            {
                "op": "v_proj",
                "kernel": "matmul_gemv_subgroup.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.v_proj"
            },
            {
                "op": "rope_q",
                "kernel": "rope.wgsl",
                "entry": "main"
            },
            {
                "op": "rope_k",
                "kernel": "rope.wgsl",
                "entry": "main"
            },
            {
                "op": "attention",
                "kernel": "attention_decode_chunked_f16kv.wgsl",
                "entry": "main"
            },
            {
                "op": "o_proj",
                "kernel": "matmul_gemv_subgroup.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.o_proj"
            },
            {
                "op": "attn_residual",
                "kernel": "residual.wgsl",
                "entry": "main"
            },
            {
                "op": "post_attn_norm",
                "kernel": "rmsnorm.wgsl",
                "entry": "main"
            },
            {
                "op": "ffn_gate_up",
                "kernel": "fused_ffn.wgsl",
                "entry": "main",
                "weights": "layer.{L}.mlp",
                "constants": {
                    "ACTIVATION": 1
                }
            },
            {
                "op": "down_proj",
                "kernel": "matmul_gemv_subgroup.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.mlp.down_proj"
            },
            {
                "op": "ffn_residual",
                "kernel": "residual.wgsl",
                "entry": "main"
            }
        ]
    },
    "prefill": {
        "steps": [
            {
                "op": "input_norm",
                "kernel": "rmsnorm.wgsl",
                "entry": "main"
            },
            {
                "op": "q_proj",
                "kernel": "matmul_f16w_f32a.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.q_proj"
            },
            {
                "op": "k_proj",
                "kernel": "matmul_f16w_f32a.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.k_proj"
            },
            {
                "op": "v_proj",
                "kernel": "matmul_f16w_f32a.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.v_proj"
            },
            {
                "op": "rope_q",
                "kernel": "rope.wgsl",
                "entry": "main"
            },
            {
                "op": "rope_k",
                "kernel": "rope.wgsl",
                "entry": "main"
            },
            {
                "op": "attention",
                "kernel": "attention_streaming_f16kv.wgsl",
                "entry": "main"
            },
            {
                "op": "o_proj",
                "kernel": "matmul_f16w_f32a.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.o_proj"
            },
            {
                "op": "attn_residual",
                "kernel": "residual.wgsl",
                "entry": "main"
            },
            {
                "op": "post_attn_norm",
                "kernel": "rmsnorm.wgsl",
                "entry": "main"
            },
            {
                "op": "ffn_gate_up",
                "kernel": "fused_ffn.wgsl",
                "entry": "main_batched",
                "weights": "layer.{L}.mlp",
                "constants": {
                    "ACTIVATION": 1
                }
            },
            {
                "op": "down_proj",
                "kernel": "matmul_f16w_f32a.wgsl",
                "entry": "main",
                "weights": "layer.{L}.mlp.down_proj"
            },
            {
                "op": "ffn_residual",
                "kernel": "residual.wgsl",
                "entry": "main"
            }
        ]
    },
    "preLayer": [
        {
            "op": "embed",
            "kernel": "gather.wgsl",
            "entry": "main",
            "weights": "embed_tokens"
        }
    ],
    "postLayer": [
        {
            "op": "final_norm",
            "kernel": "rmsnorm.wgsl",
            "entry": "main"
        },
        {
            "op": "lm_head",
            "kernel": "matmul_gemv_subgroup.wgsl",
            "entry": "main_multicol",
            "weights": "lm_head"
        },
        {
            "op": "lm_head_prefill",
            "kernel": "matmul_f16w_f32a.wgsl",
            "entry": "main",
            "weights": "lm_head"
        }
    ],
    "sampling": [
        {
            "op": "sample",
            "kernel": "sample.wgsl",
            "entry": "sample_single_pass"
        }
    ]
}