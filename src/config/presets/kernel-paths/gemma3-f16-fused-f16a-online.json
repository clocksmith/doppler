{
    "id": "gemma3-f16-fused-f16a-online",
    "name": "Gemma 3 F16 (F16 activations, fused online)",
    "description": "Online-tuned Gemma 3 F16 path with decode lm_head retile overrides and explicitly defined Fused FFN kernels.",
    "activationDtype": "f16",
    "decode": {
        "steps": [
            {
                "op": "input_norm",
                "kernel": "rmsnorm_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "q_proj",
                "kernel": "matmul_gemv_subgroup_f16a.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.q_proj"
            },
            {
                "op": "k_proj",
                "kernel": "matmul_gemv_subgroup_f16a.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.k_proj"
            },
            {
                "op": "v_proj",
                "kernel": "matmul_gemv_subgroup_f16a.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.v_proj"
            },
            {
                "op": "rope_q",
                "kernel": "rope_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "rope_k",
                "kernel": "rope_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "attention",
                "kernel": "attention_decode_online_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "o_proj",
                "kernel": "matmul_gemv_subgroup_f16a.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.self_attn.o_proj"
            },
            {
                "op": "attn_residual",
                "kernel": "residual_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "post_attn_norm",
                "kernel": "rmsnorm_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "ffn_gate_up",
                "kernel": "fused_ffn_f16.wgsl",
                "entry": "main",
                "weights": "layer.{L}.mlp",
                "constants": {
                    "ACTIVATION": 1
                }
            },
            {
                "op": "down_proj",
                "kernel": "matmul_gemv_subgroup_f16a.wgsl",
                "entry": "main_vec4",
                "weights": "layer.{L}.mlp.down_proj"
            },
            {
                "op": "ffn_residual",
                "kernel": "residual_f16.wgsl",
                "entry": "main"
            }
        ]
    },
    "prefill": {
        "steps": [
            {
                "op": "input_norm",
                "kernel": "rmsnorm_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "q_proj",
                "kernel": "matmul_f16_tiled.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.q_proj"
            },
            {
                "op": "k_proj",
                "kernel": "matmul_f16_tiled.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.k_proj"
            },
            {
                "op": "v_proj",
                "kernel": "matmul_f16_tiled.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.v_proj"
            },
            {
                "op": "rope_q",
                "kernel": "rope_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "rope_k",
                "kernel": "rope_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "attention",
                "kernel": "attention_streaming_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "o_proj",
                "kernel": "matmul_f16_tiled.wgsl",
                "entry": "main",
                "weights": "layer.{L}.self_attn.o_proj"
            },
            {
                "op": "attn_residual",
                "kernel": "residual_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "post_attn_norm",
                "kernel": "rmsnorm_f16.wgsl",
                "entry": "main"
            },
            {
                "op": "ffn_gate_up",
                "kernel": "fused_ffn_f16.wgsl",
                "entry": "main_batched",
                "weights": "layer.{L}.mlp",
                "constants": {
                    "ACTIVATION": 1
                }
            },
            {
                "op": "down_proj",
                "kernel": "matmul_f16_tiled.wgsl",
                "entry": "main",
                "weights": "layer.{L}.mlp.down_proj"
            },
            {
                "op": "ffn_residual",
                "kernel": "residual_f16.wgsl",
                "entry": "main"
            }
        ]
    },
    "preLayer": [
        {
            "op": "embed",
            "kernel": "gather_f16.wgsl",
            "entry": "main",
            "weights": "embed_tokens"
        }
    ],
    "postLayer": [
        {
            "op": "final_norm",
            "kernel": "rmsnorm_f16.wgsl",
            "entry": "main"
        },
        {
            "op": "lm_head",
            "kernel": "matmul_gemv_subgroup_f16a.wgsl",
            "entry": "main_multicol",
            "weights": "lm_head",
            "constants": {
                "MULTICOL_COLS_PER_WG": 64,
                "MULTICOL_THREADS_PER_COL": 4
            }
        },
        {
            "op": "lm_head_prefill",
            "kernel": "matmul_f16_tiled.wgsl",
            "entry": "main",
            "weights": "lm_head"
        }
    ],
    "sampling": [
        {
            "op": "sample",
            "kernel": "sample_f16.wgsl",
            "entry": "sample_single_pass"
        }
    ]
}