{
  "id": "gemma2-q4k-fused-f16a",
  "name": "Gemma 2 Q4K Fused (F16 activations)",
  "description": "Q4K weights with fused dequant+matmul using F16 activations. Best throughput when F16 activations are enabled.",
  "activationDtype": "f16",

  "decode": {
    "steps": [
      { "op": "input_norm",   "kernel": "rmsnorm_f16.wgsl",           "entry": "main" },
      { "op": "q_proj",       "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.self_attn.q_proj", "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "k_proj",       "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.self_attn.k_proj", "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "v_proj",       "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.self_attn.v_proj", "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "rope_q",       "kernel": "rope_f16.wgsl",              "entry": "main" },
      { "op": "rope_k",       "kernel": "rope_f16.wgsl",              "entry": "main" },
      { "op": "attention",    "kernel": "attention_decode_chunked_f16.wgsl",  "entry": "main",         "constants": { "SOFTCAP": 50.0 } },
      { "op": "o_proj",       "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.self_attn.o_proj", "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "attn_residual","kernel": "residual_f16.wgsl",          "entry": "main" },
      { "op": "post_attn_norm","kernel": "rmsnorm_f16.wgsl",          "entry": "main" },
      { "op": "gate_proj",    "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.mlp.gate_proj", "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "up_proj",      "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.mlp.up_proj",   "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "activation",   "kernel": "gelu_f16.wgsl",            "entry": "main", "constants": { "HAS_GATE": true } },
      { "op": "down_proj",    "kernel": "fused_matmul_q4_multicol_f16a.wgsl",  "entry": "main_multicol_f16a", "weights": "layer.{L}.mlp.down_proj", "constants": { "WORKGROUP_SIZE": 256, "COLS_PER_WG": 32, "THREADS_PER_COL_GEMV": 8 } },
      { "op": "ffn_residual", "kernel": "residual_f16.wgsl",          "entry": "main" }
    ]
  },

  "prefill": {
    "steps": [
      { "op": "input_norm",   "kernel": "rmsnorm_f16.wgsl",              "entry": "main" },
      { "op": "q_proj",       "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.self_attn.q_proj" },
      { "op": "k_proj",       "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.self_attn.k_proj" },
      { "op": "v_proj",       "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.self_attn.v_proj" },
      { "op": "rope_q",       "kernel": "rope_f16.wgsl",                 "entry": "main" },
      { "op": "rope_k",       "kernel": "rope_f16.wgsl",                 "entry": "main" },
      { "op": "attention",    "kernel": "attention_small_f16.wgsl",            "entry": "main",      "constants": { "SOFTCAP": 50.0 } },
      { "op": "o_proj",       "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.self_attn.o_proj" },
      { "op": "attn_residual","kernel": "residual_f16.wgsl",             "entry": "main" },
      { "op": "post_attn_norm","kernel": "rmsnorm_f16.wgsl",             "entry": "main" },
      { "op": "gate_proj",    "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.mlp.gate_proj" },
      { "op": "up_proj",      "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.mlp.up_proj" },
      { "op": "activation",   "kernel": "gelu_f16.wgsl",         "entry": "main", "constants": { "HAS_GATE": true } },
      { "op": "down_proj",    "kernel": "fused_matmul_q4_batched_f16a.wgsl", "entry": "main_batched_f16a",   "weights": "layer.{L}.mlp.down_proj" },
      { "op": "ffn_residual", "kernel": "residual_f16.wgsl",             "entry": "main" }
    ]
  },

  "preLayer": [
    { "op": "embed",        "kernel": "gather_f16.wgsl",             "entry": "main",         "weights": "embed_tokens" }
  ],

  "postLayer": [
    { "op": "final_norm",   "kernel": "rmsnorm_f16.wgsl",            "entry": "main" },
    { "op": "lm_head",      "kernel": "matmul_gemv_subgroup_f16a.wgsl", "entry": "main_multicol",  "weights": "lm_head" },
    { "op": "lm_head_prefill", "kernel": "matmul_f16.wgsl",          "entry": "main",  "weights": "lm_head" }
  ],

  "sampling": [
    { "op": "softcap",      "kernel": "sample_f16.wgsl",             "entry": "apply_softcap", "constants": { "SOFTCAP": 30.0 } },
    { "op": "sample",       "kernel": "sample_f16.wgsl",             "entry": "sample_single_pass" }
  ]
}
