{
  "id": "gemma2",
  "name": "Gemma 2",
  "extends": "transformer",

  "architecture": {
    "headDim": 256
  },

  "runtime": {
    "inference": {
      "batching": {
        "batchSize": 4
      }
    }
  },

  "inference": {
    "attention": {
      "slidingWindow": 4096,
      "attnLogitSoftcapping": 50.0,
      "queryKeyNorm": false
    },
    "normalization": {
      "rmsNormWeightOffset": true,
      "rmsNormEps": 1e-6,
      "postAttentionNorm": true,
      "preFeedforwardNorm": true
    },
    "ffn": {
      "activation": "gelu"
    },
    "output": {
      "finalLogitSoftcapping": 30.0
    },
    "layerPattern": {
      "type": "alternating",
      "globalPattern": "odd"
    },
    "chatTemplate": {
      "type": "gemma"
    }
  },

  "tokenizer": {
    "bosToken": "<bos>",
    "eosTokens": ["<eos>", "<end_of_turn>"],
    "addBosToken": true
  },

  "detection": {
    "architecturePatterns": ["gemma2", "Gemma2ForCausalLM"],
    "configPatterns": {
      "attn_logit_softcapping": 50.0
    }
  }
}
