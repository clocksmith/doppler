{
  "id": "gemma2",
  "name": "Gemma 2",
  "extends": "transformer",

  "architecture": {
    "headDim": 256
  },

  "inference": {
    "attention": {
      "slidingWindow": 4096,
      "attnLogitSoftcapping": 50.0,
      "queryKeyNorm": false
    },
    "normalization": {
      "rmsNormWeightOffset": true,
      "rmsNormEps": 1e-6,
      "postAttentionNorm": true,
      "preFeedforwardNorm": true
    },
    "ffn": {
      "activation": "gelu"
    },
    "output": {
      "finalLogitSoftcapping": 30.0,
      "scaleEmbeddings": true
    },
    "layerPattern": {
      "type": "alternating",
      "globalPattern": "odd"
    },
    "chatTemplate": {
      "type": "gemma",
      "enabled": true
    },
    "kernelPaths": {
      "f16": {
        "f16": "gemma2-f16-f16a",
        "f32": "gemma2-f16-f32a"
      },
      "q4k": {
        "f16": "gemma2-q4k-fused-f16a",
        "f32": "gemma2-q4k-fused-f32a"
      }
    }
  },

  "tokenizer": {
    "bosToken": "<bos>",
    "eosTokens": ["<eos>", "<end_of_turn>"],
    "addBosToken": true
  },

  "detection": {
    "architecturePatterns": ["gemma2", "Gemma2ForCausalLM"],
    "configPatterns": {
      "attn_logit_softcapping": 50.0
    }
  }
}
