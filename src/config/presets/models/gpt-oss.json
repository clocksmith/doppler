{
  "id": "gpt_oss",
  "name": "GPT-OSS",
  "extends": "mixtral",
  "modelType": "gpt-oss",

  "architecture": {
    "vocabSize": 201088,
    "ropeTheta": 150000,
    "rmsNormEps": 1e-5
  },

  "inference": {
    "attention": {
      "slidingWindow": 128,
      "attentionBias": true
    },
    "rope": {
      "ropeTheta": 150000,
      "ropeScalingType": "yarn",
      "ropeScalingFactor": 32.0,
      "yarnBetaFast": 32.0,
      "yarnBetaSlow": 1.0,
      "yarnOriginalMaxPos": 4096
    },
    "ffn": {
      "activation": "silu",
      "gatedActivation": true
    },
    "moe": {
      "kernelProfileId": "gpt-oss-moe-v1",
      "supportedActivationDtypes": ["f16", "f32"],
      "preferredActivationDtype": "f16",
      "shapeConstraints": {
        "hiddenSizeDivisor": 32,
        "intermediateSizeDivisor": 32,
        "groupSize": 32
      }
    },
    "layerPattern": {
      "type": "alternating",
      "globalPattern": "odd",
      "period": null
    },
    "chatTemplate": {
      "type": "gpt-oss",
      "enabled": true
    }
  },

  "tokenizer": {
    "bosToken": "<|startoftext|>",
    "eosTokens": ["<|return|>"],
    "padToken": "<|endoftext|>",
    "addBosToken": true,
    "addEosToken": false,
    "hfModel": "openai/gpt-oss-20b",
    "allowArchFallback": false
  },

  "detection": {
    "architecturePatterns": ["gptoss", "gpt-oss", "GptOssForCausalLM", "gpt_oss"],
    "modelTypePatterns": ["gpt_oss", "gpt-oss"],
    "configPatterns": {
      "num_local_experts": 32
    }
  }
}
