{
  "id": "embeddinggemma",
  "name": "EmbeddingGemma (Gemma 3 Text Embedding)",
  "extends": "transformer",
  "modelType": "embedding",

  "architecture": {
    "headDim": 256,
    "ropeTheta": 1000000
  },

  "inference": {
    "attention": {
      "attnLogitSoftcapping": null,
      "queryKeyNorm": true,
      "causal": false
    },
    "normalization": {
      "rmsNormWeightOffset": true,
      "rmsNormEps": 1e-6,
      "postAttentionNorm": true,
      "preFeedforwardNorm": true,
      "postFeedforwardNorm": true
    },
    "ffn": {
      "activation": "gelu"
    },
    "output": {
      "finalLogitSoftcapping": null,
      "scaleEmbeddings": true,
      "tieWordEmbeddings": false
    },
    "layerPattern": {
      "type": "every_n",
      "period": 6
    },
    "rope": {
      "ropeTheta": 1000000,
      "ropeLocalTheta": 10000
    },
    "chatTemplate": {
      "enabled": false
    },
    "kernelPaths": {
      "f16": {
        "f16": "embeddinggemma-f16-f32a",
        "f32": "embeddinggemma-f16-f32a"
      },
      "q4k": {
        "f16": "embeddinggemma-q4k-dequant-f32a",
        "f32": "embeddinggemma-q4k-dequant-f32a"
      }
    }
  },

  "tokenizer": {
    "bosToken": "<bos>",
    "eosTokens": ["<eos>", "<end_of_turn>"],
    "addBosToken": true
  },

  "detection": {
    "architecturePatterns": [
      "gemma3textmodel",
      "Gemma3TextModel",
      "embeddinggemma"
    ],
    "modelTypePatterns": [
      "embeddinggemma"
    ]
  }
}
