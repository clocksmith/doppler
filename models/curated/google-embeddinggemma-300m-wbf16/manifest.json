{
  "version": 1,
  "modelId": "google-embeddinggemma-300m-wbf16",
  "modelType": "embedding",
  "quantization": "BF16",
  "quantizationInfo": {
    "weights": "bf16",
    "embeddings": "bf16",
    "compute": "f32",
    "variantTag": "wbf16"
  },
  "architecture": {
    "numLayers": 24,
    "hiddenSize": 768,
    "intermediateSize": 1152,
    "numAttentionHeads": 3,
    "numKeyValueHeads": 1,
    "headDim": 256,
    "vocabSize": 262144,
    "maxSeqLen": 2048,
    "ropeTheta": 1000000
  },
  "moeConfig": null,
  "inference": {
    "presetId": "embeddinggemma",
    "attention": {
      "queryPreAttnScalar": 256,
      "attnLogitSoftcapping": null,
      "slidingWindow": 512,
      "queryKeyNorm": true,
      "causal": false,
      "attentionBias": false
    },
    "normalization": {
      "rmsNormEps": 0.000001,
      "rmsNormWeightOffset": true,
      "postAttentionNorm": true,
      "preFeedforwardNorm": true,
      "postFeedforwardNorm": true
    },
    "ffn": {
      "activation": "gelu",
      "gatedActivation": true,
      "swigluLimit": null
    },
    "rope": {
      "ropeTheta": 1000000,
      "ropeLocalTheta": 10000,
      "ropeScalingType": null,
      "ropeScalingFactor": 1,
      "yarnBetaFast": null,
      "yarnBetaSlow": null,
      "yarnOriginalMaxPos": null
    },
    "output": {
      "finalLogitSoftcapping": null,
      "tieWordEmbeddings": true,
      "scaleEmbeddings": true,
      "embeddingTranspose": false,
      "embeddingVocabSize": 262144
    },
    "layerPattern": {
      "type": "every_n",
      "globalPattern": null,
      "period": 6,
      "offset": 5
    },
    "chatTemplate": {
      "type": null,
      "enabled": false
    },
    "pipeline": null,
    "defaultKernelPath": "embeddinggemma-f16-f32a"
  },
  "shards": [
    {
      "index": 0,
      "filename": "shard_00000.bin",
      "size": 67108864,
      "hash": "5100ef70c204a232c3faadc15a012eecfdbff1c697d5fd49a422020f1c4cb6ab",
      "offset": 0
    },
    {
      "index": 1,
      "filename": "shard_00001.bin",
      "size": 67108864,
      "hash": "90b8dff3e0035521130544f27574d415013467869da43890936c04b5d0dd6aec",
      "offset": 67108864
    },
    {
      "index": 2,
      "filename": "shard_00002.bin",
      "size": 67108864,
      "hash": "8c971c59205390bdaf9b391a603eea7e4ea6bcaa5d05cd406b38fbd719a6e721",
      "offset": 134217728
    },
    {
      "index": 3,
      "filename": "shard_00003.bin",
      "size": 67108864,
      "hash": "c32b9b7d4d384a78aefd8aeadafbf0c4a807e1d92dcf2ae0de3d68611d552f42",
      "offset": 201326592
    },
    {
      "index": 4,
      "filename": "shard_00004.bin",
      "size": 67108864,
      "hash": "fc5a746f34e729422818d403ab4c24dedbceca60e516f07d211d45eb8e9c3977",
      "offset": 268435456
    },
    {
      "index": 5,
      "filename": "shard_00005.bin",
      "size": 67108864,
      "hash": "3e75bc34b217c6cde5afe5616a5203263e556f9b33df58e0218093d6b5ece43d",
      "offset": 335544320
    },
    {
      "index": 6,
      "filename": "shard_00006.bin",
      "size": 67108864,
      "hash": "e289b366fa454b494e207c5c45ab50e59d36411bf218125e737c5629845197f2",
      "offset": 402653184
    },
    {
      "index": 7,
      "filename": "shard_00007.bin",
      "size": 67108864,
      "hash": "9b3738812e7d50998f8e756110a8a032dbeef6daf942d65a0d32ce164edcc4fa",
      "offset": 469762048
    },
    {
      "index": 8,
      "filename": "shard_00008.bin",
      "size": 67108864,
      "hash": "9ba209fedebb1c405576646058749ff8700a842c6bbaf2ae28fc1b6a346f792f",
      "offset": 536870912
    },
    {
      "index": 9,
      "filename": "shard_00009.bin",
      "size": 1746432,
      "hash": "4b6a81cd339c8dc26d1a03d10284a397a2f3e9a452c32cb4e424262317b135ea",
      "offset": 603979776
    }
  ],
  "tensors": {
    "embed_tokens.weight": {
      "spans": [
        {
          "shardIndex": 0,
          "offset": 0,
          "size": 67108864
        },
        {
          "shardIndex": 1,
          "offset": 0,
          "size": 67108864
        },
        {
          "shardIndex": 2,
          "offset": 0,
          "size": 67108864
        },
        {
          "shardIndex": 3,
          "offset": 0,
          "size": 67108864
        },
        {
          "shardIndex": 4,
          "offset": 0,
          "size": 67108864
        },
        {
          "shardIndex": 5,
          "offset": 0,
          "size": 67108864
        }
      ],
      "size": 402653184,
      "shape": [
        262144,
        768
      ],
      "dtype": "BF16",
      "role": "embedding"
    },
    "layers.0.input_layernorm.weight": {
      "shard": 6,
      "offset": 0,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.0.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 1536,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.0.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 1771008,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.0.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 3540480,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.0.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 5309952,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.0.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 5311488,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.0.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 5313024,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.0.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 5314560,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.0.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 5315072,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.0.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 5708288,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.0.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 6887936,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.0.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 6888448,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.0.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 8068096,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.input_layernorm.weight": {
      "shard": 6,
      "offset": 8461312,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.1.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 8462848,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 10232320,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 12001792,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 13771264,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.1.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 13772800,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.1.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 13774336,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.1.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 13775872,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.1.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 13776384,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 14169600,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 15349248,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.1.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 15349760,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.1.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 16529408,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.input_layernorm.weight": {
      "shard": 6,
      "offset": 16922624,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.10.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 16924160,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 18693632,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 20463104,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 22232576,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.10.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 22234112,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.10.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 22235648,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.10.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 22237184,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.10.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 22237696,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 22630912,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 23810560,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.10.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 23811072,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.10.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 24990720,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.input_layernorm.weight": {
      "shard": 6,
      "offset": 25383936,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.11.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 25385472,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 27154944,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 28924416,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 30693888,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.11.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 30695424,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.11.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 30696960,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.11.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 30698496,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.11.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 30699008,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 31092224,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 32271872,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.11.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 32272384,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.11.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 33452032,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.input_layernorm.weight": {
      "shard": 6,
      "offset": 33845248,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.12.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 33846784,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 35616256,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 37385728,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 39155200,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.12.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 39156736,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.12.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 39158272,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.12.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 39159808,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.12.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 39160320,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 39553536,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 40733184,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.12.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 40733696,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.12.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 41913344,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.input_layernorm.weight": {
      "shard": 6,
      "offset": 42306560,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.13.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 42308096,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 44077568,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 45847040,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 47616512,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.13.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 47618048,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.13.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 47619584,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.13.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 47621120,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.13.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 47621632,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 48014848,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 49194496,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.13.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 49195008,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.13.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 50374656,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.input_layernorm.weight": {
      "shard": 6,
      "offset": 50767872,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.14.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 50769408,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 52538880,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 54308352,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 56077824,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.14.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 56079360,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.14.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 56080896,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.14.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 56082432,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.14.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 56082944,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 56476160,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 57655808,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.14.self_attn.q_proj.weight": {
      "shard": 6,
      "offset": 57656320,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.14.self_attn.v_proj.weight": {
      "shard": 6,
      "offset": 58835968,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.input_layernorm.weight": {
      "shard": 6,
      "offset": 59229184,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.15.mlp.down_proj.weight": {
      "shard": 6,
      "offset": 59230720,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.mlp.gate_proj.weight": {
      "shard": 6,
      "offset": 61000192,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.mlp.up_proj.weight": {
      "shard": 6,
      "offset": 62769664,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.post_attention_layernorm.weight": {
      "shard": 6,
      "offset": 64539136,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.15.post_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 64540672,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.15.pre_feedforward_layernorm.weight": {
      "shard": 6,
      "offset": 64542208,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.15.self_attn.k_norm.weight": {
      "shard": 6,
      "offset": 64543744,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.15.self_attn.k_proj.weight": {
      "shard": 6,
      "offset": 64544256,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.self_attn.o_proj.weight": {
      "shard": 6,
      "offset": 64937472,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.self_attn.q_norm.weight": {
      "shard": 6,
      "offset": 66117120,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.15.self_attn.q_proj.weight": {
      "spans": [
        {
          "shardIndex": 6,
          "offset": 66117632,
          "size": 991232
        },
        {
          "shardIndex": 7,
          "offset": 0,
          "size": 188416
        }
      ],
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.15.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 188416,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.input_layernorm.weight": {
      "shard": 7,
      "offset": 581632,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.16.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 583168,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 2352640,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 4122112,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 5891584,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.16.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 5893120,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.16.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 5894656,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.16.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 5896192,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.16.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 5896704,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 6289920,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 7469568,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.16.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 7470080,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.16.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 8649728,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.input_layernorm.weight": {
      "shard": 7,
      "offset": 9042944,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.17.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 9044480,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 10813952,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 12583424,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 14352896,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.17.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 14354432,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.17.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 14355968,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.17.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 14357504,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.17.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 14358016,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 14751232,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 15930880,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.17.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 15931392,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.17.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 17111040,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.input_layernorm.weight": {
      "shard": 7,
      "offset": 17504256,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.18.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 17505792,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 19275264,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 21044736,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 22814208,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.18.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 22815744,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.18.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 22817280,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.18.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 22818816,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.18.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 22819328,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 23212544,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 24392192,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.18.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 24392704,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.18.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 25572352,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.input_layernorm.weight": {
      "shard": 7,
      "offset": 25965568,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.19.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 25967104,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 27736576,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 29506048,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 31275520,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.19.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 31277056,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.19.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 31278592,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.19.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 31280128,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.19.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 31280640,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 31673856,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 32853504,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.19.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 32854016,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.19.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 34033664,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.input_layernorm.weight": {
      "shard": 7,
      "offset": 34426880,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.2.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 34428416,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 36197888,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 37967360,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 39736832,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.2.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 39738368,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.2.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 39739904,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.2.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 39741440,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.2.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 39741952,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 40135168,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 41314816,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.2.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 41315328,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.2.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 42494976,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.input_layernorm.weight": {
      "shard": 7,
      "offset": 42888192,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.20.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 42889728,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 44659200,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 46428672,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 48198144,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.20.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 48199680,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.20.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 48201216,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.20.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 48202752,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.20.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 48203264,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 48596480,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 49776128,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.20.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 49776640,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.20.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 50956288,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.input_layernorm.weight": {
      "shard": 7,
      "offset": 51349504,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.21.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 51351040,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 53120512,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 54889984,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 56659456,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.21.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 56660992,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.21.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 56662528,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.21.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 56664064,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.21.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 56664576,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 57057792,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 58237440,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.21.self_attn.q_proj.weight": {
      "shard": 7,
      "offset": 58237952,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.21.self_attn.v_proj.weight": {
      "shard": 7,
      "offset": 59417600,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.input_layernorm.weight": {
      "shard": 7,
      "offset": 59810816,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.22.mlp.down_proj.weight": {
      "shard": 7,
      "offset": 59812352,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.mlp.gate_proj.weight": {
      "shard": 7,
      "offset": 61581824,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.mlp.up_proj.weight": {
      "shard": 7,
      "offset": 63351296,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.post_attention_layernorm.weight": {
      "shard": 7,
      "offset": 65120768,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.22.post_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 65122304,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.22.pre_feedforward_layernorm.weight": {
      "shard": 7,
      "offset": 65123840,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.22.self_attn.k_norm.weight": {
      "shard": 7,
      "offset": 65125376,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.22.self_attn.k_proj.weight": {
      "shard": 7,
      "offset": 65125888,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.self_attn.o_proj.weight": {
      "shard": 7,
      "offset": 65519104,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.self_attn.q_norm.weight": {
      "shard": 7,
      "offset": 66698752,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.22.self_attn.q_proj.weight": {
      "spans": [
        {
          "shardIndex": 7,
          "offset": 66699264,
          "size": 409600
        },
        {
          "shardIndex": 8,
          "offset": 0,
          "size": 770048
        }
      ],
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.22.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 770048,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.input_layernorm.weight": {
      "shard": 8,
      "offset": 1163264,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.23.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 1164800,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 2934272,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 4703744,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 6473216,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.23.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 6474752,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.23.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 6476288,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.23.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 6477824,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.23.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 6478336,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 6871552,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 8051200,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.23.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 8051712,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.23.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 9231360,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.input_layernorm.weight": {
      "shard": 8,
      "offset": 9624576,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.3.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 9626112,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 11395584,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 13165056,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 14934528,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.3.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 14936064,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.3.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 14937600,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.3.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 14939136,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.3.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 14939648,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 15332864,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 16512512,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.3.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 16513024,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.3.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 17692672,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.input_layernorm.weight": {
      "shard": 8,
      "offset": 18085888,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.4.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 18087424,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 19856896,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 21626368,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 23395840,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.4.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 23397376,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.4.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 23398912,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.4.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 23400448,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.4.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 23400960,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 23794176,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 24973824,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.4.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 24974336,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.4.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 26153984,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.input_layernorm.weight": {
      "shard": 8,
      "offset": 26547200,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.5.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 26548736,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 28318208,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 30087680,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 31857152,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.5.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 31858688,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.5.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 31860224,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.5.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 31861760,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.5.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 31862272,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 32255488,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 33435136,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.5.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 33435648,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.5.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 34615296,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.input_layernorm.weight": {
      "shard": 8,
      "offset": 35008512,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.6.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 35010048,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 36779520,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 38548992,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 40318464,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.6.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 40320000,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.6.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 40321536,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.6.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 40323072,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.6.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 40323584,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 40716800,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 41896448,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.6.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 41896960,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.6.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 43076608,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.input_layernorm.weight": {
      "shard": 8,
      "offset": 43469824,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.7.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 43471360,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 45240832,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 47010304,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 48779776,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.7.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 48781312,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.7.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 48782848,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.7.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 48784384,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.7.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 48784896,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 49178112,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 50357760,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.7.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 50358272,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.7.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 51537920,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.input_layernorm.weight": {
      "shard": 8,
      "offset": 51931136,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.8.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 51932672,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 53702144,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 55471616,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 57241088,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.8.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 57242624,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.8.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 57244160,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.8.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 57245696,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.8.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 57246208,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.self_attn.o_proj.weight": {
      "shard": 8,
      "offset": 57639424,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.self_attn.q_norm.weight": {
      "shard": 8,
      "offset": 58819072,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.8.self_attn.q_proj.weight": {
      "shard": 8,
      "offset": 58819584,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.8.self_attn.v_proj.weight": {
      "shard": 8,
      "offset": 59999232,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.input_layernorm.weight": {
      "shard": 8,
      "offset": 60392448,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.9.mlp.down_proj.weight": {
      "shard": 8,
      "offset": 60393984,
      "size": 1769472,
      "shape": [
        768,
        1152
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.mlp.gate_proj.weight": {
      "shard": 8,
      "offset": 62163456,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.mlp.up_proj.weight": {
      "shard": 8,
      "offset": 63932928,
      "size": 1769472,
      "shape": [
        1152,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.post_attention_layernorm.weight": {
      "shard": 8,
      "offset": 65702400,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.9.post_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 65703936,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.9.pre_feedforward_layernorm.weight": {
      "shard": 8,
      "offset": 65705472,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.9.self_attn.k_norm.weight": {
      "shard": 8,
      "offset": 65707008,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.9.self_attn.k_proj.weight": {
      "shard": 8,
      "offset": 65707520,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.self_attn.o_proj.weight": {
      "spans": [
        {
          "shardIndex": 8,
          "offset": 66100736,
          "size": 1008128
        },
        {
          "shardIndex": 9,
          "offset": 0,
          "size": 171520
        }
      ],
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.self_attn.q_norm.weight": {
      "shard": 9,
      "offset": 171520,
      "size": 512,
      "shape": [
        256
      ],
      "dtype": "BF16",
      "role": "norm"
    },
    "layers.9.self_attn.q_proj.weight": {
      "shard": 9,
      "offset": 172032,
      "size": 1179648,
      "shape": [
        768,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "layers.9.self_attn.v_proj.weight": {
      "shard": 9,
      "offset": 1351680,
      "size": 393216,
      "shape": [
        256,
        768
      ],
      "dtype": "BF16",
      "role": "matmul"
    },
    "norm.weight": {
      "shard": 9,
      "offset": 1744896,
      "size": 1536,
      "shape": [
        768
      ],
      "dtype": "BF16",
      "role": "norm"
    }
  },
  "totalSize": 605726208,
  "hashAlgorithm": "blake3",
  "eos_token_id": 1,
  "metadata": {
    "source": "convert-core",
<<<<<<< Updated upstream
    "convertedAt": "2026-02-22T17:54:33.363Z",
=======
    "convertedAt": "2026-02-22T17:27:25.151Z",
>>>>>>> Stashed changes
    "hasTokenizer": true
  },
  "tokenizer": {
    "type": "bundled",
    "vocabSize": 3119,
    "file": "tokenizer.json"
  }
}